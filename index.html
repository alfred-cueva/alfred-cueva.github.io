<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Alfred Cueva</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20100%20100'%3E%3Ctext%20y='.9em'%20font-size='90'%3E%F0%9F%8C%90%3C/text%3E%3C/svg%3E">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Alfred Cueva 
                </p>
                <p style="text-align:justify">
                I am a Masters Student in Robotics & AI at <a href="https://www.gatech.edu/" target="_blank" rel="noopener noreferrer"> Georgia Tech</a>. Currently, I am a Researcher at the <a href="https://lab-idar.gatech.edu/" target="_blank" rel="noopener noreferrer"> LIDAR </a> Lab supervised by <a href="https://www.me.gatech.edu/faculty/zhao" target="_blank" rel="noopener noreferrer"> Prof. Ye Zhao</a> and <a href="https://www.linkedin.com/in/zhaoyuangu/" target="_blank" rel="noopener noreferrer"> Zhaoyuan Gu</a> on learning-based control and motion planning for embodied AI. My work spans the full robotics pipeline from algorithm design and simulation to real-world deployment, developing scalable sim-to-real frameworks for loco-manipulation tasks in real world environments.   
              
                </p>
                <p style="text-align:justify">
                Previously, I worked at <a href="https://www.samsungcnt.com/eng/index.do" target="_blank" rel="noopener noreferrer">Samsung</a> as a Robotics & ML Software Engineer for 2 years, where I led end-to-end development of autonomous systems, from perception and motion planning to real-world deployment on various robots. I received my Bachelors in Mechanical Engineering from <a href="https://en.snu.ac.kr/index.html" target="_blank" rel="noopener noreferrer">Seoul National University</a> during which I was a Research Intern at    
                <a href="http://dyros.snu.ac.kr/" target="_blank" rel="noopener noreferrer">Dynamic Robotics System Lab</a>, advised by <a href="https://scholar.google.co.kr/citations?user=XtKmE78AAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Prof. Jaeheung Park</a> and <a href="https://daegyulim.github.io/" target="_blank" rel="noopener noreferrer">Dr. Daegyu Lim</a> focusing on model-based priors guided Reinforcement Learning for legged robots. I also interned at <a href = "https://softrobotics.snu.ac.kr" target="_blank" rel="noopener noreferrer">Soft Robotics & Bionics Lab</a> under the guidance of <a href="https://softrobotics.snu.ac.kr/people.php" target="_blank" rel="noopener noreferrer">Prof. Yong-Lae Park</a>  on soft robotic multi-modal sensing for industrial robots.

                <p style="text-align:justify">
                I also co-organized a non-profit organization dedicated to AI education, <a href="https://www.youtube.com/channel/UCfmSTxHQ6Y43XtHsQ7l_H3Q" target="_blank" rel="noopener noreferrer">AI Tech Play</a>, and hosted the first nationwide AI camp focused on autonomous racing competitions for high school students.
                </p>

                <p style="text-align:justify">
                <strong style="color: red;">I'm currently looking for internships, feel free to reach out!</strong>
                </p>
              
 
                <p style="text-align:justify">
                <span id="fun-fact">Fun Fact: I am fluent in 4 languages.</span>
                </p>
                <p style="text-align:justify">
                <button onclick="showFunFact()">Show another fun fact</button>
                </p>
                  
                <p style="text-align:center">
                  <a href="mailto:alfred.cueva@gatech.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Alfred_Resume_Dec30.pdf" target="_blank" rel="noopener noreferrer">Resume</a> &nbsp;/&nbsp;
                  
                  <a href="https://www.linkedin.com/in/alfred-cueva/" target="_blank" rel="noopener noreferrer">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/alfred-cueva/" target="_blank" rel="noopener noreferrer">Github</a> &nbsp;/&nbsp; 
                  <a href="https://x.com/alfie14x" target="_blank" rel="noopener noreferrer"> X </a> &nbsp;/&nbsp; 
                  <a href="https://www.instagram.com/alfred11x/" target="_blank" rel="noopener noreferrer"> Instagram </a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/Headshot_final_circle.PNG" class="hoverZoomLink">   
              </td>
            </tr>


          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Current Position</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%; border: 0; border-spacing: 0; margin: auto; border-collapse: separate;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/gt_logo.png" alt="Georgia Tech" class="company-logo">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.gatech.edu/" target="_blank" rel="noopener noreferrer" style="font-size:16px;font-weight:bold;">Georgia Institute of Technology</a>
                  <br>
                  <span class="papertitle">Graduate Researcher - LIDAR Lab</span> | <a href="https://lab-idar.gatech.edu/" target="_blank" rel="noopener noreferrer">Laboratory for Intelligent Decision and Autonomous Robots</a>
                  <br>
                  <em>Aug 2024 - Present</em>
                  <p style="text-align:justify">
                    Researching generalizable robot policies for long-horizon, contact-rich tasks, leveraging Reinforcement Learning, diffusion models, motion planning, whole-body control, and sim-to-real transfer for humanoid loco-manipulation.
                    <br><br>
                    Designed hierarchical control framework combining diffusion policies with RL fine-tuning to improve success rates on complex humanoid loco-manipulation tasks (<strong>Submitted to RA-L/IROS</strong>).
                    <br><br>
                    Developed Dockerized cloud infrastructure and custom Isaac Lab environments for 31-DOF humanoid, defining physics models, rewards, and observation/action spaces.
                    <br><br>
                    Built VR teleoperation and demonstration pipeline for scalable data collection supporting imitation and reinforcement learning.
                    <br><br>
                    Created USD-based high-fidelity simulation workflows, validating sim-to-real transfer of whole-body stabilization and fine manipulation policies on hardware achieving 85% success on door opening, object transport (up to 5kg), and dynamic climbing on Booster platform.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · Isaac Sim/Lab · USD · Docker · VR Systems · MuJoCo · Diffusion Models · PPO/SAC</em></p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%; border: 0; border-spacing: 0; margin: auto; border-collapse: separate;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/samsung_logo.png" alt="Samsung" class="company-logo">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.samsungcnt.com/eng/index.do" target="_blank" rel="noopener noreferrer" style="font-size:16px;font-weight:bold;">Samsung</a>
                  <br>
                  <span class="papertitle">Robotics & ML Software Engineer</span>
                  <br>
                  <em>Mar 2024 - Aug 2025 (1 yr 6 mos)</em>
                  <p style="text-align:justify">
                    Spearheaded production-grade YOLOv8 <a href="https://biz.chosun.com/en/en-realestate/2025/08/19/OVQ3Q57EHFGRVAX5CO4SU2VKUY/" target="_blank" rel="noopener noreferrer">perception system</a> for mobile robots, achieving 92% detection accuracy through end-to-end dataset creation (10K+ images), training, on-device optimization, and CI/CD integration for continuous deployment.
                    <br><br>
                    Led development of precision control and SLAM-based localization for 7-DOF manipulator, reducing positioning error by 15% and earning <strong>$10,000 award</strong> in the <a href="https://news.samsungcnt.com/en/features/engineering-construction/2024-11-samsung-campt-recognized-with-two-top-innovation-awards-at-the-2024-smart-construction-challenge/" target="_blank" rel="noopener noreferrer">Smart Construction Challenge</a> from Korean Government.
                    <br><br>
                    Engineered ROS2 autonomous navigation stack for mobile robots, combining RRT* and Hybrid A* planning with real-time perception to enable robust decision-making in dynamic, cluttered environments.
                    <br><br>
                    Built <a href="https://biz.chosun.com/en/en-realestate/2025/08/19/OVQ3Q57EHFGRVAX5CO4SU2VKUY/" target="_blank" rel="noopener noreferrer">Isaac Sim digital twin workflows</a> for synthetic data generation, domain randomization, and sensor simulation. Led cross-team integration to validate sim-to-real transfer across perception, manipulation, navigation, and control modules.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · ROS2 · Isaac Sim · YOLOv8 · SLAM · Embedded ML · CI/CD</em></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/samsung_logo.png" alt="Samsung" class="company-logo">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.samsungcnt.com/eng/index.do" target="_blank" rel="noopener noreferrer" style="font-size:16px;font-weight:bold;">Samsung</a>
                  <br>
                  <span class="papertitle">Computer Vision Intern</span>
                  <br>
                  <em>Jul 2023 - Aug 2023 (2 mos)</em>
                  <p style="text-align:justify">
                    Developed real-time heat anomaly detection system for semiconductor manufacturing equipment, designing custom ML model architectures and GPU-accelerated data pipelines to process large-scale multi-sensor streams for low-latency decision-making.
                    <br><br>
                    Integrated perception-driven, collision-aware planning for autonomous cluttered-bin retrieval, deploying and optimizing Segment Anything (SAM) on industrial edge hardware to achieve significant inference speedups while maintaining segmentation accuracy.
                    <br><br>
                    Engineered sensor-fusion based collision avoidance system for AGVs, combining IMU and multi-modal perception data to enable robust, real-time navigation in dynamic, safety-critical industrial environments.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · SAM · OpenCV · GPU Optimization · Sensor Fusion · Real-Time Systems</em></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/snu_logo.png" alt="SNU DYROS" class="company-logo">
                </td>
                <td width="75%" valign="middle">
                  <a href="http://dyros.snu.ac.kr/" target="_blank" rel="noopener noreferrer" style="font-size:16px;font-weight:bold;">Seoul National University</a>
                  <br>
                  <span class="papertitle">Research Assistant - Dynamic Robotics Systems Lab</span>, advised by <a href="https://scholar.google.co.kr/citations?user=XtKmE78AAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Prof. Jaeheung Park</a>
                  <br>
                  <em>Dec 2022 - Feb 2024</em> | <strong style="color: red;">Outstanding Thesis Award (1/120)</strong>
                  <p style="text-align:justify">
                    Proposed novel model-free RL sim-to-real framework for energy-efficient whole-body control, achieving 19% speed improvement and 22% energy cost reduction in bipedal robot. Designed meta-RL optimization framework combining model-free learning with physics-based actuator models to optimize energy consumption and parallel elastic actuator parameters for weak actuation scenarios.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>MuJoCo · PyTorch · MAML · Model-Based RL</em></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/snu_logo.png" alt="SNU Soft Robotics" class="company-logo">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://softrobotics.snu.ac.kr" target="_blank" rel="noopener noreferrer" style="font-size:16px;font-weight:bold;">Seoul National University</a>
                  <br>
                  <span class="papertitle">Research Intern - Soft Robotics & Bionics Lab</span>, advised by <a href="https://softrobotics.snu.ac.kr/people.php" target="_blank" rel="noopener noreferrer">Prof. Yong-Lae Park</a>
                  <br>
                  <em>Mar 2021 - Aug 2021</em>
                  <p style="text-align:justify">
                    Developed software and system integration for capacitive touch-sensing grid used as force-control interface for industrial sewing robots. Improved robot responsiveness and control precision, increasing sewing operation speed by 20% through real-time feedback optimization.
                    <br><br>
                    Designed and calibrated sensor-fusion algorithms to convert multi-point pressure measurements into stable, low-latency control signals. Collaborated cross-functionally with hardware, controls, and fabrication teams to prototype and validate soft robotic end-to-end Software-in-the-Loop (SIL) pipeline.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>Python · C++ · Sensor Fusion · Capacitive Sensing · Real-Time Control</em></p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research centers on robot learning for contact-rich manipulation in real-world environments. I develop scalable frameworks that combine physics-aware learning with learned representations for long-horizon reasoning, enabling robots to acquire dexterous and agile motor skills. Ultimately, I aim to bridge the gap between human and robot capabilities—empowering machines to perform complex tasks in unpredictable settings with human-like adaptability.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%; border: 0; border-spacing: 0; margin: auto; border-collapse: separate;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/DPRL.gif" alt="critical" width="200" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="" target="_blank" rel="noopener noreferrer">
                    <span class="papertitle"> Humanoid Loco-manipulation via Joint Optimization of Diffusion Policy and Reinforcement Learning</span>
                  </a>
                  <br>
                  <!-- Zhaoyuan Gu*, Yipu Chen*, Zimeng Chai, Alfred Cueva, Ye Zhao  -->
                  <strong>Under Review for RA-L</strong>
                  <br>
                  <p style="text-align:justify">
                    Developed joint optimization framework combining diffusion policies with reinforcement learning for humanoid loco-manipulation. The approach fine-tunes offline diffusion policies with online RL interaction, adapting to new scenarios beyond training data. Achieved 85% success rate on door opening, box transport (up to 5kg), and table climbing tasks on Booster humanoid platform. System demonstrates robust whole-body coordination in contact-rich scenarios beyond training distribution.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · MuJoCo · IsaacGym · Diffusion Models · PPO · Whole-Body Control · Sim-to-Real</em></p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Drilling_demo.gif" alt="critical" width="200" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.youtube.com/watch?v=Atvf2HXZ59k&t=1s" target="_blank" rel="noopener noreferrer">
                    <span class="papertitle"> Autonomous Drilling Robot for Cluttered environments</span>
                  </a>
                  <br>
                  Project at <a href="https://www.secc.co.kr/en/library/tech/smart" target="_blank" rel="noopener noreferrer"> Samsung </a> (<strong style="color: red;">Smart Construction Robotics Challenge Winner</strong>)
                  <p style="text-align:justify">
                    Led development of autonomous drilling robot for construction sites, deploying rule-based computer vision and motion planning for precise surface drilling with ±2mm accuracy. System handles 100+ kg payloads and operates in GPS-denied cluttered environments, reducing human exposure to hazardous tasks by 80%. Deployed across 5+ construction sites.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>ROS2 · OpenCV · Motion Planning · Force Control</em></p>
                  <p>[<a href="https://news.samsungcnt.com/en/features/engineering-construction/2024-11-samsung-campt-recognized-with-two-top-innovation-awards-at-the-2024-smart-construction-challenge/" target="_blank" rel="noopener noreferrer">Coverage</a>]
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/navifra.gif" alt="critical" width="200" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.mk.co.kr/en/realestate/11360686" target="_blank" rel="noopener noreferrer">
                    <span class="papertitle"> Adaptive AMR for Safe and Efficient Material Transport</span>
                  </a>
                  <br>
                  Project at <a href="https://www.secc.co.kr/en/library/tech/smart" target="_blank" rel="noopener noreferrer"> Samsung,</a> in collaboration with <a href="https://www.hd-hyundairobotics.com/en/main" target="_blank" rel="noopener noreferrer">Hyundai Robotics</a>
                  <p style="text-align:justify">
                    Led development and deployment of adaptive AMR fleet (300+ robots) for material transport in construction sites. Implemented safety-aware navigation achieving 99.2% successful delivery rate in dynamic, GPS-denied environments. System handles 200kg payloads with real-time obstacle avoidance and multi-robot coordination.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>ROS2 · LiDAR SLAM · Nav2 · Fleet Management · Motion Planning</em></p>
                  <p>[<a href="https://biz.chosun.com/en/en-realestate/2025/08/19/OVQ3Q57EHFGRVAX5CO4SU2VKUY/" target="_blank" rel="noopener noreferrer">Coverage</a>]
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px; width: 25%; vertical-align: middle;">
                  <img src="images/Thesis_cover.PNG" alt="critical" width="200" height="160">
                </td>
                <td style="width: 75%; vertical-align: middle;">
                  <a href="https://github.com/alfred-cueva/Undergraduate-Thesis" target="_blank" rel="noopener noreferrer">
                    <span class="papertitle">RL-Policy Guided Optimal Design of Parallel Elastic Actuator for Weak Actuation of Bipedal Robot</span>
                  </a>
                  <br>
                  <strong>Alfred Cueva</strong>, <a href="https://scholar.google.co.kr/citations?user=XtKmE78AAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Jaeheung Park</a>, <a href="https://softrobotics.snu.ac.kr/people.php" target="_blank" rel="noopener noreferrer">Yong-Lae Park</a>
                  <br>
                  Bachelor's Thesis (<strong style="color: red;">Outstanding BS Thesis Presentation Award</strong>)
                  <p style="text-align:justify">
                    Developed meta-RL optimization framework combining model-free learning with physics-based actuator models for bipedal locomotion. Achieved 40% energy reduction in simulated bipedal walking while optimizing parallel elastic actuator stiffness parameters. Framework demonstrated successful sim-to-real transfer potential for weak actuation scenarios.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>MuJoCo · PyTorch · MAML · Model-Based RL</em></p>
                  <p>[<a href="https://github.com/alfred-cueva/Undergraduate-Thesis" target="_blank" rel="noopener noreferrer">Code</a> | <a href="data/Thesis_AlfredCueva.pdf" target="_blank" rel="noopener noreferrer">Report</a>]</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Projects</h2>
              </td>
            </tr>
          </tbody>
          </table>

          <table style="width:100%; border: 0; border-spacing: 0; margin: auto; border-collapse: separate;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/diffusionpolicydemo.gif" alt="Diffusion Policy Demo" width="200" height="160">
                </td>
                <td width="75%" valign="middle">
                <a href="https://github.com/alfred-cueva/lerobot/tree/gatech/reproduce_results" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle">Diffusion Policy for Multi-Step Block Assembly</span>
                </a>
                <br>
                <strong>Alfred Cueva*</strong>,
                <a href="https://kr.linkedin.com/in/seok-joon-kim" target="_blank" rel="noopener noreferrer">Seok Joon Kim*</a>,
                <a href="https://www.linkedin.com/in/kyle-kam" target="_blank" rel="noopener noreferrer">Kyle Kam*</a>
                <br>
                <em>Graduate Course Project (<a href="https://www.pair.toronto.edu/cs8803-drl-f25/" target="_blank" rel="noopener noreferrer">Deep Reinforcement Learning</a>, Fall 2025)</em>
                <br>
                <p style="text-align:justify">Implemented transformer-based diffusion policies for long-horizon visuomotor manipulation, focusing on sequential pick-and-place and multi-block assembly. Integrated camera-calibrated ArUco-based 6DoF pose estimation with action diffusion for closed-loop execution on UR10e robot. Achieved 85% success rate on 3-block assembly tasks with 200+ teleoperated expert demonstrations.</p>
                <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · LeRobot · ROS · OpenCV · ArUco Markers · UR10e</em></p>
                <p>[<a href="https://drive.google.com/file/d/1PM5gxX7_wvLej2E9k4NQKgBkHQ0dRpS6/view?usp=drive_link" target="_blank" rel="noopener noreferrer">Report</a> | <a href="https://drive.google.com/file/d/1AYaX9hzdl1SUnnqsHHyjTSODT1o1i9rP/view?usp=drive_link" target="_blank" rel="noopener noreferrer">Video</a> | <a href="https://github.com/alfred-cueva/Diffusion-Policy" target="_blank" rel="noopener noreferrer">Code</a>]</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <video src="images/turtlebotracing.mp4" alt="Turtlebot Demo" style="width:200px;height:160px;object-fit:cover;" controls autoplay muted loop></video>
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/alfred-cueva/Intro-to-Autonomy" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle">Multi-Modal Perception for Autonomous Maze Navigation</span>
                </a>
                <br>
                <strong>Alfred Cueva*</strong>,
                <a href="https://de.linkedin.com/in/cgaetal" target="_blank" rel="noopener noreferrer">Carlos Gaeta*</a>
                <br>
                <em>Graduate Course Project (Introduction to Autonomy, Fall 2025)</em>
                <br>
                <p style="text-align:justify">Designed end-to-end ROS2 autonomous navigation stack integrating LiDAR-based Bug0 obstacle avoidance with vision-based traffic sign recognition using KNN classifier. Deployed on TurtleBot3 platform for autonomous maze navigation with real-time sign detection and adaptive waypoint generation. Achieved 95% sign recognition accuracy across 10+ maze configurations.</p>
                <p style="color:#666;font-size:12px;margin-top:8px;"><em>ROS · Python · OpenCV · TensorFlow · LiDAR · TurtleBot3</em></p>
                <p>[<a href="https://github.com/alfred-cueva/Intro-to-Autonomy" target="_blank" rel="noopener noreferrer">Code</a>]</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cartpole_example.PNG" alt="pgm" width="200" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/alfred-cueva/RL-Agent-for-Rapid-Task-Adaptation" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle"> Reinforcement Learning Agent for Rapid Task Adaptation </span>
                </a>
                <br>
                <strong>Alfred Cueva*</strong>, <a href="https://www.linkedin.com/in/gene-chung-988573303/?originalSubdomain=kr" target="_blank" rel="noopener noreferrer">Gene Chung*</a>,
                  Taehung Kim*, Sumin Ye*
                <br>
                <em>Graduate Course Project (Reinforcement Learning, Spring 2023)</em>
                <br>
                  <p style="text-align:justify">
                    Developed Task-Invariant Agent (TIA) network for multi-task RL, enabling rapid adaptation to new tasks using model dynamics. The architecture integrates a modified DQN policy network, an encoder for latent task representation from experience sequences, and a model predictor for system dynamics. Achieved 3x faster adaptation to new reward functions compared to baseline DQN, demonstrating robust generalization across CartPole task variants.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · OpenAI Gym · DQN · Meta-Learning</em></p>
                  <p>[<a href="https://github.com/alfred-cueva/RL-Agent-for-Rapid-Task-Adaptation" target="_blank" rel="noopener noreferrer">Code</a> | <a href="https://github.com/alfred-cueva/RL-Agent-for-Rapid-Task-Adaptation/blob/main/report.pdf" target="_blank" rel="noopener noreferrer">Report</a>  ]</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/2DBPP_example.PNG" alt="pgm" width="200" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/alfred-cueva/2D-BPP-RL" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle"> Constrained 2D Online Bin Packing Problem using Reinforcement Learning</span>
                </a>
                <br>
                <strong>Alfred Cueva</strong>
                <br>
                <em>Graduate Course Project (Combinatorial Optimization, Spring 2023)</em>
                <br>
                <p style="text-align:justify">
                  Implemented Heuristics Integrated Deep RL approach for online 2D bin packing with placement constraints. Trained PPO agent to learn optimal packing strategies that outperform traditional heuristics. Achieved 15% improvement in space utilization over baseline greedy algorithms.
                </p>
                <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · PPO · OpenAI Gym · Combinatorial Optimization</em></p>
                <p>[<a href="https://github.com/alfred-cueva/2D-BPP-RL" target="_blank" rel="noopener noreferrer">Code</a> | <a href="https://github.com/alfred-cueva/2D-BPP-RL/blob/main/Report.pdf" target="_blank" rel="noopener noreferrer">Report</a>]</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/humanoid_example.png" alt="pgm" width="200" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/alfred-cueva/Theory-and-Practice-of-Humanoid-Walking-Control" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle"> Control Techniques for Humanoid Robots </span>
                </a>
                <br>
                <strong>Alfred Cueva</strong>
                <br>
                <em>Graduate Course Project (Theory and Practice of Humanoid Walking Control, Fall 2022)</em>
                <br>
                  <p style="text-align:justify">
                    Implemented 10+ humanoid control algorithms including ZMP-based walking pattern generation, Linear Inverted Pendulum Model, preview control, and whole-body operational space control. Developed CoM estimation using complementary filters and capture point-based stabilization for dynamic walking on simulated bipedal robots.
                  </p>
                  <p style="color:#666;font-size:12px;margin-top:8px;"><em>C++ · MATLAB · Whole-Body Control · QP Solvers · Trajectory Optimization</em></p>
                  <p>[<a href="https://github.com/alfred-cueva/Theory-and-Practice-of-Humanoid-Walking-Control" target="_blank" rel="noopener noreferrer">Code</a>]</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/RCcar_example.png" alt="pgm" width="200" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/alfred-cueva/RL-Race-Car/tree/main" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle"> RC Car Autonomous Driving  </span>
                </a>
                <br>
                <strong>Alfred Cueva*</strong>, <a href="https://mx.linkedin.com/in/maxacostamtz" target="_blank" rel="noopener noreferrer">Max Acosta*</a>
                <br>
                <em>Graduate Course Project (<a href="https://rllab.snu.ac.kr/courses/intelligent-systems_2023" target="_blank" rel="noopener noreferrer">Introduction to Intelligent Systems</a>, Spring 2023)</em>
                <br>
                <p style="text-align:justify">
                  Developed autonomous driving system for RC Car Racing Challenge using LiDAR-only perception for mapless navigation. Implemented behavior cloning with Gaussian Process Regression to learn driving policy from expert demonstrations. Trained end-to-end control policy mapping raw sensor observations to steering and throttle commands. Achieved top-3 finish in class competition with average lap speed of 2.5 m/s while maintaining safe wall clearance of 15cm.
                </p>
                <p style="color:#666;font-size:12px;margin-top:8px;"><em>Python · PyTorch · LiDAR · Gaussian Process · Behavior Cloning</em></p>
                <p>[<a href="https://github.com/alfred-cueva/RL-Race-Car" target="_blank" rel="noopener noreferrer">Code</a>]</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Soil_demo.gif" alt="critical" width="200" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/alfred-cueva/ML_based_soil_remote_sensing" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle"> Soil Sensing with Machine Learning and Satellite Imagery </span>
                </a>
                <br>
                <strong>Alfred Cueva*</strong>, <a href="https://www.linkedin.com/in/dykm" target="_blank" rel="noopener noreferrer">Andy Kim*</a>
                <br>
                <em>Graduate Course Project (<a href="https://viplab.snu.ac.kr/viplab/courses/mldl1_2023_2/index.html" target="_blank" rel="noopener noreferrer">Deep Learning</a>, Fall 2023) <strong style="color: red;">(Funded by National Research Foundation)</strong></em>
                <p style="text-align:justify">
                  Led research estimating soil health from satellite imagery for agricultural policy enforcement. Managed $8K grant and directed field surveys across 50+ sites, collecting 100GB of GIS and satellite data. Engineered 30+ novel features from multi-spectral satellite data and GIS sources, training machine learning models (XGBoost, Random Forest) for regression. Achieved 60% improvement over baseline estimates (R² = 0.78) using ensemble methods.
                </p>
                <p style="color:#666;font-size:12px;margin-top:8px;"><em>Python · Scikit-learn · XGBoost · Sentinel-2 · GIS</em></p>
                <p>[<a href="https://github.com/alfred-cueva/ML_based_soil_remote_sensing" target="_blank" rel="noopener noreferrer">Code</a>]</p>

              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/SpiritNet_example.png" alt="pgm" width="200" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="data/SpiritNet_Report.pdf" target="_blank" rel="noopener noreferrer">
                  <span class="papertitle"> Video Generation from Single-Image Input </span>
                </a>
                <br>
                <strong>Alfred Cueva</strong>
                <br>
                <em>Graduate Course Project (Computer Vision, Fall 2021)</em>
                <br>
                <p style="text-align:justify">Developed novel video generation algorithm that synthesizes realistic video sequences from a single input image using sequential structure learning. Integrated optical flow estimation with temporal consistency constraints to eliminate awkward motion artifacts common in frame-by-frame generation approaches.</p>
                <p style="color:#666;font-size:12px;margin-top:8px;"><em>PyTorch · Optical Flow · CNNs · Video Synthesis</em></p>
                <p>[<a href="data/SpiritNet_Report.pdf" target="_blank" rel="noopener noreferrer">Report</a>]</p>
              </td>
            </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px 20px 0px 20px;width:100%;vertical-align:middle">
                <h2>Technical Skills</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:10px 20px 20px 20px;width:100%;vertical-align:middle">
                <p style="margin:5px 0;\"><strong>Robotics & Control:</strong> ROS/ROS2, MuJoCo, IsaacGym, Isaac Sim/Lab, USD, Gazebo, MoveIt, Whole-Body Control, Model Predictive Control, Trajectory Optimization</p>
                <p style="margin:5px 0;\"><strong>Machine Learning:</strong> PyTorch, TensorFlow, Diffusion Policy, Reinforcement Learning (PPO, SAC, TD3), Imitation Learning, Model-Based RL, Meta-Learning</p>
                <p style="margin:5px 0;\"><strong>Computer Vision:</strong> OpenCV, YOLO, SAM, NeRF Pipelines, 3D Reconstruction, Visual SLAM, Camera Calibration, ArUco Markers, Depth Estimation</p>
                <p style="margin:5px 0;\"><strong>Programming & Tools:</strong> Python, C++, MATLAB, Git, Docker, Kubernetes, Linux, CUDA, CI/CD, Real-Time Systems, Embedded Systems</p>
                <p style="margin:5px 0;\"><strong>Hardware & Platforms:</strong> Booster T1, Unitree G1, Digit, Franka Panda, UR10e, Tabletop Manipulators, AMRs, LiDAR Systems, RGB-D Cameras, VR Systems</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:0px">
                  <p style="text-align:right;font-size:small;">
                    <span style="float:left;">
                      <a href="#top"><i class="fas fa-chevron-up fa-2x"></i></a>
                    </span>
                    Template from <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody></table>
          </td>
        </tr>
      </table>
    <script>
      const funFacts = [
        "Fluent in four languages.",
        "Lived in five different countries.",
        "Opening bottles with paper is easier than you think.",
        "New York cheesecake is a guilt-free pleasure.",
        "Camped in the Sahara Desert for a night.",
        "Caught a stunning sunset at Bangkok’s tallest tower",
        "Solo-traveled through Southeast Asia for a month.",
        "Trained every other day for a month to run a half marathon.",
        "Spent part of my childhood in the Amazon rainforest.",
        "Jazz bars are my favorite places to unwind."
      ];

      function showFunFact() {
        let current = document.getElementById('fun-fact').textContent.replace('Fun Fact: ', '');
        let facts = funFacts.filter(f => f !== current);
        const fact = facts[Math.floor(Math.random() * facts.length)];
        document.getElementById('fun-fact').textContent = 'Fun Fact: ' + fact;
      }
    </script>
    </body>
    
    </html>
